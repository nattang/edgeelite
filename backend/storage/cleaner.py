"""
Text cleaning and processing for session data.
Contains placeholder functions for cleaning agent integration.
"""
import re
from typing import List, Dict, Any, Tuple


def basic_text_cleaning(text: str) -> str:
    """
    Perform basic text cleaning operations.
    
    Args:
        text: Raw text to clean
        
    Returns:
        Cleaned text
    """
    if not text:
        return ""
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text)
    
    # Strip leading/trailing whitespace
    text = text.strip()
    
    # Remove non-printable characters
    text = re.sub(r'[^\x20-\x7E]', '', text)
    
    return text

##TODO: Implement this function properly
def align_ocr_audio_by_timestamp(events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Align OCR and audio events by timestamp for better context.
    
    Args:
        events: List of raw events with timestamps
        
    Returns:
        List of aligned events
    """
    # Sort events by timestamp
    sorted_events = sorted(events, key=lambda x: x['ts'])
    
    # Basic cleaning for each event
    for event in sorted_events:
        event['text'] = basic_text_cleaning(event['text'])
    
    return sorted_events

##TODO: Implement this function properly -> With some LLM processing
def clean_session_data(events: List[Dict[str, Any]]) -> Tuple[str, str]:
    """
    PLACEHOLDER: Clean and process session data using an LLM/agent.
    
    This function will be replaced with actual LLM-based cleaning logic.
    The LLM will:
    1. Align OCR and audio content contextually
    2. Remove duplicates and noise
    3. Create a coherent narrative from the session
    4. Generate a 1-2 line summary
    
    Args:
        events: List of raw events from a session
        
    Returns:
        Tuple of (summary, full_cleaned_data)
    """
    if not events:
        return "", ""
    
    # Align events by timestamp
    aligned_events = align_ocr_audio_by_timestamp(events)
    
    # Group events by source for context
    ocr_texts = []
    audio_texts = []
    
    for event in aligned_events:
        if event['source'] == 'ocr':
            ocr_texts.append(event['text'])
        elif event['source'] == 'audio':
            audio_texts.append(event['text'])
    
    # PLACEHOLDER: This would be replaced with actual LLM processing
    # For now, we'll do basic concatenation and summarization
    
    # Combine all text
    all_text = []
    if ocr_texts:
        ocr_combined = ' '.join(ocr_texts)
        all_text.append(f"Screen content: {ocr_combined}")
    
    if audio_texts:
        audio_combined = ' '.join(audio_texts)
        all_text.append(f"Audio content: {audio_combined}")
    
    full_data = ' | '.join(all_text)
    
    # Create a basic summary (placeholder)
    # This would be generated by the LLM
    if len(full_data) > 100:
        summary = f"Session containing {len(ocr_texts)} screen captures and {len(audio_texts)} audio segments"
    else:
        summary = full_data[:50] + "..." if len(full_data) > 50 else full_data
    
    return summary, full_data


def preprocess_for_embedding(text: str) -> str:
    """
    Preprocess text before embedding generation.
    
    Args:
        text: Text to preprocess
        
    Returns:
        Preprocessed text optimized for embedding
    """
    if not text:
        return ""
    
    # Basic cleaning
    text = basic_text_cleaning(text)
    
    # Normalize case for better embedding consistency
    text = text.lower()
    
    # Remove excessive punctuation
    text = re.sub(r'[^\w\s.,!?-]', '', text)
    
    # Limit length for embedding model
    if len(text) > 512:
        text = text[:512]
    
    return text 