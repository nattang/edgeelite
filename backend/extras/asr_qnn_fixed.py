#!/usr/bin/env python3
"""
Fixed QNN ASR Service for EdgeElite Backend - NPU Optimized
Working version that doesn't hang during decoder inference
"""

import numpy as np
import onnxruntime as ort
import soundfile as sf
import librosa
import os
import json
import time

BASE_DIR = os.path.dirname(__file__)

# Model paths for QNN-optimized models
ENCODER_PATH = os.path.join(BASE_DIR, "models/whisper_large_v3_turbo-hfwhisperencoder-qualcomm_snapdragon_x_elite.onnx/model.onnx/model.onnx")
DECODER_PATH = os.path.join(BASE_DIR, "models/whisper_large_v3_turbo-hfwhisperdecoder-qualcomm_snapdragon_x_elite.onnx/model.onnx/model.onnx")

# Tokenizer files
MODEL_DIR = os.path.join(BASE_DIR, "models/asr")
VOCAB_PATH = os.path.join(MODEL_DIR, "vocab.json")
ADDED_TOKENS_PATH = os.path.join(MODEL_DIR, "added_tokens.json")
SPECIAL_TOKENS_PATH = os.path.join(MODEL_DIR, "special_tokens_map.json")

# Audio settings
SAMPLING_RATE = 16000
CHUNK_DURATION = 5.0

class QNNASRServiceFixed:
    """Fixed ASR Service optimized for Qualcomm Snapdragon X-Elite NPU."""
    
    def __init__(self):
        """Initialize the QNN ASR service."""
        self.encoder_sess = None
        self.decoder_sess = None
        self.token_to_id = {}
        self.id_to_token = {}
        self.special_tokens = {}
        
        # Check QNN availability
        self.qnn_available = 'QNNExecutionProvider' in ort.get_available_providers()
        print(f"üöÄ QNN NPU available: {self.qnn_available}")
        
        # Load models and tokenizer
        self._load_models()
        self._load_tokenizer()
    
    def _load_models(self):
        """Load encoder and decoder models with QNN optimization."""
        try:
            print("üîß Loading ASR models with QNN NPU...")
            
            # Set up providers with QNN priority
            providers = ['CPUExecutionProvider']  # Fallback
            if self.qnn_available:
                providers.insert(0, 'QNNExecutionProvider')
                print("üöÄ Using QNN NPU for ASR inference!")
            else:
                print("‚ö†Ô∏è QNN NPU not available, using CPU fallback")
            
            # Check if model files exist
            if not os.path.exists(ENCODER_PATH):
                raise FileNotFoundError(f"Encoder model not found: {ENCODER_PATH}")
            if not os.path.exists(DECODER_PATH):
                raise FileNotFoundError(f"Decoder model not found: {DECODER_PATH}")
            
            # Load encoder
            print(f"üì• Loading encoder from: {ENCODER_PATH}")
            try:
                self.encoder_sess = ort.InferenceSession(ENCODER_PATH, providers=providers)
                print(f"‚úÖ Encoder loaded with providers: {self.encoder_sess.get_providers()}")
            except Exception as encoder_error:
                print(f"‚ùå Encoder loading failed: {encoder_error}")
                # Try with CPU only
                print("üîÑ Retrying encoder with CPU only...")
                self.encoder_sess = ort.InferenceSession(ENCODER_PATH, providers=['CPUExecutionProvider'])
                print(f"‚úÖ Encoder loaded with CPU fallback: {self.encoder_sess.get_providers()}")
            
            # Load decoder
            print(f"üì• Loading decoder from: {DECODER_PATH}")
            try:
                self.decoder_sess = ort.InferenceSession(DECODER_PATH, providers=providers)
                print(f"‚úÖ Decoder loaded with providers: {self.decoder_sess.get_providers()}")
            except Exception as decoder_error:
                print(f"‚ùå Decoder loading failed: {decoder_error}")
                # Try with CPU only
                print("üîÑ Retrying decoder with CPU only...")
                self.decoder_sess = ort.InferenceSession(DECODER_PATH, providers=['CPUExecutionProvider'])
                print(f"‚úÖ Decoder loaded with CPU fallback: {self.decoder_sess.get_providers()}")
            
            # Show model specifications
            print("\nüìã Model Specifications:")
            print("Encoder inputs:")
            for inp in self.encoder_sess.get_inputs():
                print(f"  {inp.name}: {inp.shape} ({inp.type})")
            
            print("Decoder inputs:")
            for inp in self.decoder_sess.get_inputs():
                print(f"  {inp.name}: {inp.shape} ({inp.type})")
                
        except Exception as e:
            print(f"‚ùå Model loading failed: {e}")
            raise
    
    def _load_tokenizer(self):
        """Load tokenizer files."""
        try:
            print("üìö Loading tokenizer...")
            
            # Load vocabulary
            with open(VOCAB_PATH, "r", encoding="utf-8") as f:
                self.token_to_id = json.load(f)
            
            # Load added tokens
            try:
                with open(ADDED_TOKENS_PATH, "r", encoding="utf-8") as f:
                    added_tokens = json.load(f)
                self.token_to_id.update(added_tokens)
            except (json.JSONDecodeError, FileNotFoundError):
                print("‚ö†Ô∏è added_tokens.json missing, using empty dict")
                added_tokens = {}
            
            # Load special tokens
            with open(SPECIAL_TOKENS_PATH, "r", encoding="utf-8") as f:
                special_tokens_map = json.load(f)
            
            # Create reverse mapping
            self.id_to_token = {v: k for k, v in self.token_to_id.items()}
            
            # Create special tokens dict
            self.special_tokens = {
                token: self.token_to_id[token] 
                for token in special_tokens_map.get("additional_special_tokens", []) 
                if token in self.token_to_id
            }
            self.special_tokens.update(added_tokens)
            
            print(f"‚úÖ Tokenizer loaded with {len(self.token_to_id)} tokens")
            
        except Exception as e:
            print(f"‚ùå Tokenizer loading failed: {e}")
            raise
    
    def preprocess_audio(self, audio):
        """Preprocess audio for Whisper input."""
        # Resample if needed
        if len(audio.shape) > 1:
            audio = np.mean(audio, axis=1)
        
        # Normalize
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio)) * 0.95
        
        return audio
    
    def log_mel_spectrogram(self, audio, n_mels=128):
        """Generate log mel spectrogram for Whisper."""
        n_fft = 400
        hop_length = 160
        
        # Pad audio
        audio = np.pad(audio, (0, n_fft), mode='constant')
        
        # Generate mel spectrogram
        mel_spec = librosa.feature.melspectrogram(
            y=audio,
            sr=SAMPLING_RATE,
            n_fft=n_fft,
            hop_length=hop_length,
            n_mels=n_mels,
            fmin=0,
            fmax=8000,
            power=2.0,
            window='hann',
            center=True,
            pad_mode='reflect'
        )
        
        # Convert to log scale
        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
        log_mel_spec = np.clip(log_mel_spec, -80, 0)
        log_mel_spec = (log_mel_spec + 80) / 80 * 2 - 1
        
        return log_mel_spec.astype(np.float32)
    
    def pad_or_trim_mel(self, mel, target_len=3000):
        """Pad or trim mel spectrogram to target length."""
        if mel.shape[1] > target_len:
            return mel[:, :target_len]
        elif mel.shape[1] < target_len:
            pad_width = target_len - mel.shape[1]
            return np.pad(mel, ((0, 0), (0, pad_width)), mode="constant", constant_values=-1.0)
        return mel
    
    def run_encoder(self, audio_chunk):
        """Run encoder inference on audio chunk."""
        try:
            # Preprocess audio
            audio = self.preprocess_audio(audio_chunk)
            
            # Generate mel spectrogram
            mel = self.log_mel_spectrogram(audio)
            mel = self.pad_or_trim_mel(mel, 3000)
            mel = mel[np.newaxis, :, :]
            
            # Convert to float16 for QNN optimization
            mel = mel.astype(np.float16)
            
            print(f"üß† Running encoder with QNN NPU...")
            print(f"   Input shape: {mel.shape}, dtype: {mel.dtype}")
            
            start_time = time.time()
            encoder_out = self.encoder_sess.run(None, {"input_features": mel})[0]
            encoder_time = time.time() - start_time
            
            print(f"‚úÖ Encoder completed in {encoder_time:.3f}s")
            print(f"   Output shape: {encoder_out.shape}")
            
            return encoder_out
            
        except Exception as e:
            print(f"‚ùå Encoder inference failed: {e}")
            return None
    
    def run_decoder_fixed(self, encoder_out, max_tokens=15):
        """Fixed decoder inference that doesn't hang."""
        try:
            print(f"üß† Running decoder with QNN NPU...")
            
            # Initialize with start token
            tokens = [self.special_tokens.get("<|startoftranscript|>", 50257)]
            
            start_time = time.time()
            
            # Simplified decoder approach - use greedy decoding with early stopping
            for step in range(max_tokens):
                # Check for timeout (5 seconds max for decoder)
                if time.time() - start_time > 5.0:
                    print("‚ö†Ô∏è Decoder timeout, returning partial result")
                    break
                
                # Prepare inputs for decoder - simplified version
                input_ids = np.array([tokens], dtype=np.int32)
                position_ids = np.array([len(tokens) - 1], dtype=np.int32)
                
                # Create simplified attention mask
                attention_mask = np.ones((1, 1, 1, 200), dtype=np.float16)
                
                # Create cache inputs - simplified
                k_cache_self = []
                v_cache_self = []
                k_cache_cross = []
                v_cache_cross = []
                
                for i in range(4):
                    k_cache_self.append(np.zeros((20, 1, 64, 199), dtype=np.float16))
                    v_cache_self.append(np.zeros((20, 1, 199, 64), dtype=np.float16))
                    k_cache_cross.append(encoder_out)  # Use encoder output directly
                    v_cache_cross.append(encoder_out.transpose(0, 1, 3, 2))
                
                # Prepare decoder inputs
                inputs = {
                    "input_ids": input_ids,
                    "attention_mask": attention_mask,
                    "position_ids": position_ids,
                    "k_cache_self_0_in": k_cache_self[0],
                    "v_cache_self_0_in": v_cache_self[0],
                    "k_cache_self_1_in": k_cache_self[1],
                    "v_cache_self_1_in": v_cache_self[1],
                    "k_cache_self_2_in": k_cache_self[2],
                    "v_cache_self_2_in": v_cache_self[2],
                    "k_cache_self_3_in": k_cache_self[3],
                    "v_cache_self_3_in": v_cache_self[3],
                    "k_cache_cross_0": k_cache_cross[0],
                    "v_cache_cross_0": v_cache_cross[0],
                    "k_cache_cross_1": k_cache_cross[1],
                    "v_cache_cross_1": v_cache_cross[1],
                    "k_cache_cross_2": k_cache_cross[2],
                    "v_cache_cross_2": v_cache_cross[2],
                    "k_cache_cross_3": k_cache_cross[3],
                    "v_cache_cross_3": v_cache_cross[3]
                }
                
                # Run decoder inference with timeout protection
                try:
                    outputs = self.decoder_sess.run(None, inputs)
                    logits = outputs[0]
                except Exception as run_error:
                    print(f"‚ùå Decoder inference step {step} failed: {run_error}")
                    break
                
                # Process logits - simplified
                logits = logits[0, -1, :]
                next_token = int(np.argmax(logits))
                
                tokens.append(next_token)
                
                # Check for end tokens
                if next_token == self.special_tokens.get("<|endoftext|>", 50256):
                    break
                if next_token == self.special_tokens.get("<|nospeech|>", 50361):
                    break
                
                # Early stopping if we have enough tokens
                if len(tokens) > 10:
                    break
            
            # Decode tokens to text
            content_tokens = [t for t in tokens[1:] if t < 50000]
            text_tokens = [self.id_to_token.get(tid, f"[UNK_{tid}]") for tid in content_tokens]
            text = "".join(text_tokens).replace("ƒ†", " ").replace("‚ñÅ", " ").strip()
            text = " ".join(text.split())
            
            decoder_time = time.time() - start_time
            print(f"‚úÖ Decoder completed in {decoder_time:.3f}s, generated: '{text}'")
            return text
            
        except Exception as e:
            print(f"‚ùå Decoder inference failed: {e}")
            return ""
    
    def transcribe_audio(self, audio_file):
        """Transcribe audio file using QNN NPU."""
        try:
            print(f"üé§ Transcribing: {audio_file}")
            
            # Load audio
            audio, sr = sf.read(audio_file)
            if sr != SAMPLING_RATE:
                audio = librosa.resample(audio, orig_sr=sr, target_sr=SAMPLING_RATE)
            
            # Check audio quality
            if np.sqrt(np.mean(audio**2)) < 0.001:
                print("‚ö†Ô∏è Audio seems very quiet")
            
            # Process audio in chunks
            chunk_size = int(SAMPLING_RATE * CHUNK_DURATION)
            results = []
            
            for i in range(0, len(audio), chunk_size):
                chunk = audio[i:i + chunk_size]
                if len(chunk) < chunk_size // 4:
                    continue
                
                if len(chunk) < chunk_size:
                    chunk = np.pad(chunk, (0, chunk_size - len(chunk)), mode='constant')
                
                start_time = i / SAMPLING_RATE
                end_time = min((i + chunk_size) / SAMPLING_RATE, len(audio) / SAMPLING_RATE)
                
                print(f"\nüìù Processing chunk {i//chunk_size + 1} ({start_time:.1f}s - {end_time:.1f}s)")
                
                # Run encoder
                encoder_out = self.run_encoder(chunk)
                if encoder_out is None:
                    continue
                
                # Run decoder with fixed implementation
                text = self.run_decoder_fixed(encoder_out)
                
                results.append({
                    "start": round(start_time, 2),
                    "end": round(end_time, 2),
                    "text": text
                })
            
            print(f"\nüéâ Transcription complete! Found {len(results)} segments")
            return results
            
        except Exception as e:
            print(f"‚ùå Transcription failed: {e}")
            return []

def main():
    """Test the fixed QNN ASR service."""
    print("üöÄ EdgeElite ASR Service - QNN NPU Optimized (FIXED)")
    print("=" * 50)
    
    try:
        # Initialize ASR service
        asr = QNNASRServiceFixed()
        
        # Test with existing audio file
        audio_file = "../recordings/audio-2025-07-13T07-04-08-677Z.wav"
        
        if os.path.exists(audio_file):
            print(f"\nüéµ Testing with: {audio_file}")
            results = asr.transcribe_audio(audio_file)
            
            print("\nüìã Transcription Results:")
            for result in results:
                print(f"  {result['start']}s - {result['end']}s: {result['text']}")
        else:
            print(f"‚ùå Audio file not found: {audio_file}")
            
    except Exception as e:
        print(f"‚ùå ASR service failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 